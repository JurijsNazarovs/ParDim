--------------------------------------------------------------------------------
[Introduction]
--------------------------------------------------------------------------------
ParDim - software, which provides the framework (front end) for a simple
procedure of creating pipeline by integrating different stages and paralleling
the execution for a number of analyzed directories, based on HTCondor
environment.

In other words, if you have a script to run on condor for one directory,
which corresponds to a specific data set or experiment, ParDim implements runs
for multiple directories.
Note: more about script is in section [Script designing for ParDim].

ParDim provides tools to manage a workflow of scripts, download and prepare data
for runs, and a great reporting function to get status of running jobs.

Ð¡onstruction of a pipeline is completed in an intuitive description of
stages in a text file, with ability to include/exclude a specific stage by
changing a value of just one argument - execute. Such structure allows to
keep all information in one text file and controll the workflow without
deleting chunks of parameters. As an example, pipeline with stage1 -> stage3
looks like:

--------------------
##[ stage1 ]##
execute    true
...
##[ stage2 ]##
execute    false
...
##[ stage3 ]##
execute    true
...
--------------------

ParDim provides all necessary information about a previous stage of pipeline
to a next stage using text files. It controls that results of astage are
transferred in a right directory in an organized way (according to a stage name).

--------------------------------------------------------------------------------
[ParDim installation and execution]
--------------------------------------------------------------------------------
----------------------------------------
*INSTALLATION*
----------------------------------------
1. To install the ParDim go in the directory where you would like to install
   software:
   $ softDir="/path/to/softwareDirectory"
   $ cd "$softDir"
2. Download the current version from the GitHub repository:
   $ git clone git@github.com:JurijsNazarovs/ParDim.git
3. Add following lines in your .bash_profile file (should be in your $HOME path):
   softDir="/path/to/softwareDirectory"
   export PATH="$softDir/ParDim/:$PATH"

Now you have an access to 3 function from any directory:
1. ParDim.sh - ParDim framework
2. MakeArgsFile.sh - creates a template for an argument file
3. MakeReport.sh - provides an information about pipeline running status

----------------------------------------
*EXECUTION*
----------------------------------------
In this subsection we assumne that you did actions described in the subsection
*INSTALLATION* above
--------------------
ParDim.sh
--------------------
ParDim.sh is a bash script and executed as:
ParDim.sh "argsFile" "isSubmit"

1. argsFile - path to a text file with constructed pipeline and relative
   arguments.

   Note: default value is args.list in a root directory of ParDim.sh
2. isSubmit  - boolean variable with values true/false.
   If value is false, then everything is prepared to run the pipeline, but
   does not run, to test the structure to make sure everything is ok.

   Note: default value is true, which means run the whole pipeline.

--------------------
MakeArgsFile.sh
--------------------
MakeArgsFile.sh is a bash script and executed as:
MakeArgsFile.sh "argsFile" "isAppend" "stage1" ... "stageN"

1. argsFile - path to a text file with constructed pipeline and relative
   arguments.

   Note: default value is args.list in a root directory of ParDim.sh
2. isAppend - boolean variable with values true/false.
   true  - append to existing argsFile without creating a head for ParDim.sh
   false - rewrite the whole file
3-. different names of stages in order of required execution

   Note: default value is just ParDim stage, to create necessary arguments
   to run ParDim. More details in section [ParDim Stage].

Note: MakeArgsFile.sh creates a template for every stage, which has to be
filled by user.
Note: for the stage with name Download, ParDim creates different set of
arguments, since it is a built in function.

--------------------
MakeReport.sh
--------------------
MakeReport.sh is a bash script and executed as:
MakeReport.sh "stageName" "jobsDir" "reportDir" "holdReason" "delim"

1. stageName - name of the stage of which to get summary
2. jobsDir - the working directory for the task, specified in ParDim
3. reportDir - directory to create all report files. Default is report
4. holdReason - reason for holding jobs, e.g. "" - all hold jobs, "72 hrs"
5. delim - delimeter to use for output files

The output of MakeReport.sh is 8 files:
1. *.queuedJobs.list - queued jobs
2. *.compJobs.list - completed jobs
3. *.notCompJobs.list - currently not completed jobs
4. *.holJobsReason.list - holding lines given reason $holdReason
5. *.holdJobs.list - jobs on hold given reason $holdReason
6. *.summaryJobs.list - summary info about directories
7. *.notCompDirs.list - path to not completed directories
8. *.compDirs.list - path to completed directories


--------------------------------------------------------------------------------
[Pipeline construction]
--------------------------------------------------------------------------------
To construct a pipeline - combination of stages, the argument file (text file)
is filled with specific syntax for a stage description.

----------------------------------------
*REQUIRED INPUT*
----------------------------------------
There are 3 required arguments to describe a pipeline stage
  1. stageName - provided in a pattern (*S* - any number of spaces):
     *S*##[*S*stageName*S*]##*S*
     
     Note: stageName contains no spaces!
  2. execute - boolean variable with values true/false. Indicates, if a stage
     should be executed or not. If the stage is not executed,
     it is just skipped. So, there is no need to delete it from the argument file.

     Note: if value is not true, task is not executed and warning appears.
  3. script - path to the script, which is executed to construct a DAG.
  
     Note: if not full path is provided, then path is assumed to be relative to a
     directory, where ParDim is executed.
     
----------------------------------------
*ADDITIONAL INPUT*
----------------------------------------
  1. map - argument with 2 values (now): multi/single. Tells what is the type
     of a stage. (more examples later)
  2. args - path to argument file, which is used for a script.
     If the argument is empty, then the current file with description of the
     pipeline is used for a script.
     That means, that in the description of the stage you can provide
     all other necessary information for the script, or create another file
     and provide path to that file here.
     
     Note: if not full path is provided, then path is assumed to be relative to a
     directory, where ParDim is executed.
  3. transFiles - path to all files, splited by comma (and spaces), which are used by
     a script. For example, if you have some libraries to send, you can
     add them here.
     
     Note: if not full path is provided, then the dirname of script is
     considered as a root path of transFiles.

     Note: Files corresponding to arguments "args" and "script" are transferred
     automatically.
  4. relResPath - path for results relative to the part of the pipeline or
     dataPath.
     Possible values are: previous/next stageName and dataPath.
     That is, results for current stage are saved in results directory of a
     stage specified in relResPath.
     Example: There is data in dataPath, and we need to create new additional
     data in same directory. So, that next stage can read original + new data
     from dataPath (or any stages). Then relResPath is dataPath.

     Note: if nothing is provided, then relResPath is resPath/stageName

Note: ParDim.sh provides an error if something is wrong with scripts, args,
transFiles, or relResPath.

----------------------------------------
*EXAMPLE* with all possible arguments
----------------------------------------
##[ stageName ]##  
execute                 true/false
map                     single/multi
script                  /path/to/script
args                    /path/to/argument/file/for/script
transFiles              /path/to/different/files/to/transfer/using/,
relResPath              


--------------------------------------------------------------------------------
[ParDim Stage]
--------------------------------------------------------------------------------
In argsFile, there are 4 available arguments for the ParDim to initialize
the pipeline. Variables have to be specified after the label: ##[ ParDim.sh ]##
   
1. dataPath - path to analyzed data or resulting path for download stage
   (later about that). The path is used to construct a list of analysed
   directories.
2. resPath - path to results of the whole pipeline.
   By default every stage creates its own directory corresponding
   to a stage name. But no default value for resPath.
3. jobsDir - temporary directory for all files generated by ParDim.
   Defaul is to create a temporary unique directory dagTestXXXX .
4. selectJobsListPath - initial list of directories for the analysis.
   Can be empty.

Note: in a majority of cases by providing 2 arguments: resPath and
either dataPath or selectJobsListPath, ParDim works fine and ready to construct
a pipeline. However, there are different possible combinations of arguments,
which are described below.

----------------------------------------
*POSSIBLE COMBINATIONS OF ARGUMENTS*
----------------------------------------
1. Default values - these rules are applied first, so, the values assigned
   here are used in steps 2-4
   - if [jobsDir] is empty, then a directory with unique name dagTmpXXXX
     is created
   - if [resPath] is empty, but [dataPath] is not, then resPath = dirname(dataPath)
   
2. If there is the Download task or [dataPath] is used as [relResPath] for any
   stage, then
   - [dataPath] has to be provided with ability to write
   
3. If first stage is single-mapping, then following argumetns have to be provided:
   - [resPath]
   - [jobsDir]

4. If first stage is multi-mapping, then following argumetns have to be provided:
   - [resPath]
   - [jobsDir]
   - [dataPath] or [selectJobsListPath]


--------------------------------------------------------------------------------
[Script designing for ParDim]
--------------------------------------------------------------------------------
The flow of ParDim is dedicated to 2 steps:
    - construct the DAG file (let call a script to construct a DAG file as DagMaker)
    - execute the constructed DAG file
Thus, to design a stage for ParDim, user has to provide a script - DagMaker,
which creates a DAG condor file, and ParDim handles its execution. 

Depending on a mapping value of a stage different arguments are passed to a
DagMaker (but almost the same). Thus, while designing your scripts
(DagMaker for stages), you have to write it in a way to accept these
arguments in the same order. In following subsections details for specific
stages are provided.

Note: DagMaker has to be able to read all necessary argument to  construct a
DAG from the text file. It can be done by using ParDim function ReadArgs.
More details in the subsection *USEFULL FUNCTIONS*

----------------------------------------
*SINGLE-MAPPING SCRIPTS*
----------------------------------------
single-mapping scripts are designed to construct a DAG file based on the
specific argument(file), which is most likely has to be pass using args in a
description of a stage.  Usually, it is some sort of prescritps before a
multimapping script.

--------------------
Examples:
--------------------
  1) Download files according to list of links. In this case the args is a
     path to a text file - list of links. The DAG file consists of independent
     jobs, where every job downloads file accordint to a specific link.

     Note: this is a short description of the built-in Download stage of ParDim.
     More details are in a section [Built-in Download stage].
  2) Assume we have a text file with N rows, where N rows contain information
     about M id. We would like to analyze every ID separately. In this case the
     args is a path to a text file. The DAG file consists of M independent jobs,
     where every job creates a directory and saves the part of a file
     corresponding to one of M IDs. Thus, creating an input for a multi-mapping
     stage.

--------------------
DagMaker arguments
--------------------
Following arguments are provided for a single-mapping DagMaker script by ParDim:

  1. argsFile - a text file with arguments necessary for a DagMaker.
     This is the file which you specify as a value in args in a stage description.
     If nothing is specified, the main file with arguments (where the structure
     of a pipeline is described) is used.
  2. dagFile - the name of the DAG file which is submit in a second step.
  3. jobsDir - working directory where DagMaker saves all necessary files
     for DAG submission.
  4. resPath - path where results are saved.
  
     The suggested code (to create a DAG description file) to save results
     in the "$resPath":
     conFile="$jobsDir/Name.condor"
     jobId="$uniqJobId" #can be an iterator to count jobs
     
     printf "JOB  $jobId $conFile\n" >> "$dagFile"
     printf "VARS $jobId transOut=\"$jobId.tar.gz\"\n" >> "$dagFile"
     printf "VARS $jobId transMap=\"\$(transOut)=$resPath/\$(transOut)\"\n"\
     >> "$dagFile"

     Note: the structure of a condor file and executed one is given later.
     
  5. selectJobsListInfo - file with all information about results of a previous
     stage. If previous stage exist.

     Note: Structure of file is provided later
     
----------------------------------------
*MULTI-MAPPING SCRIPTS*
----------------------------------------
Multi-mapping scripts are designed to construct a DAG file based on information
about the specific directory, so that it is executed for every of analyzed
directories independently.

--------------------
Examples:
--------------------
  1) alignment of fastq files for an experiment
  2) peak calling for an experiment
  3) trimming for an experiment

--------------------
DagMaker arguments
--------------------
Following arguments are provided for a multi-mapping DagMaker script by ParDim:

  1. argsFile - argument file which is used by dagMaker to pass all arguments.
     This is the file which you specify as a value in args in a stage description.
     So, if nothing is specified, the main file with arguments is used.
  2. dagFile - the name of the DAG file which is submit in a second step.
  3. jobsDir - working directory where DagMaker saves all necessary files
     for DAG submission.
  4. resPath - path where results are saved. Piece of code how to do that is
     provided below.
     
     The suggested code (to create a DAG description file) to save results
     in the "$resPath":
     conFile="$jobsDir/Name.condor"
     jobId="$uniqJobId" #can be an an iterator to count jobs
     # $transOut - variable described below
     
     printf "JOB  $jobId $conFile\n" >> "$dagFile"
     printf "VARS $jobId transOut=\"$transOut.jobId.tar.gz\"\n" >> "$dagFile"
     printf "VARS $jobId transMap=\"\$(transOut)=$resPath/\$(transOut)\"\n"\
     >> "$dagFile"

     Note: the structure of a condor file and executed one is given later
     
  5. inpDataInfo - file with all information about results of a previous
     stage for the same directory (specific analyzed directory and not for
     all of them as in single script)

     Note: Structure of file is provided later.
  6. resDir - directory to save results and has to be tared with following arg:
  7. transOut - an unique name of a tarfile to tar resDir

  Note: transOut is necessary to provide to create a unique tar file
  corresponding to an analysed directory, since all of resutls are collected in
  one directory and untared later by ParDim, when the stage is done.

----------------------------------------
*STRUCTURE OF A CONDOR FILE*
----------------------------------------
In both cases of maps we specify transOut and transMap as variables for
specific job. So, conFile description is:

transfer_output = $(transOut)
transfer_output_remaps = "$(transMap)"
arguments = "'arg1', 'arg2', '\$(transOut)', ..."

----------------------------------------
*STRUCTURE OF AN EXECUTION FILE*
----------------------------------------
So, in an execution file, the results have to be tared in the name corresponding
to a value in transOut variable. That is, in an execution file you have to have
a variable like outTar=$3, where $3 - corresponds to a position of transOut in 
arguments in condor file (see above). And then do:
tar -czf "$dirWithAllresults" "$jobId.tar.gz"

----------------------------------------
*STRUCTURE OF selectJobsListInfo AND inpDataInfo*
----------------------------------------
The difference between selectJobsListInfo and InpDataInfo is that
first file is a combination of a second file but for all directories in
the resulting directory of the previous stage. While the InpDataInfo contains
information form the previous stage  just about analysing directory.

The InpDataInfo structure is:
/path/to/directory/or/subdirectory:
fileName1        size in bytes
....
fileNameN        size in bytes

Note: the space above is '\t' - tabular.


--------------------------------------------------------------------------------
[ParDim built-in boost downloading stage]  [Built-in Download stage]
--------------------------------------------------------------------------------
To download files to fill the dataPath ParDim.sh provides a stage
##[ Download ]##. The name of the stage is reserved for built-in ParDim.sh script
(boostDownload.sh),
which has to be the first stage (or single) in a pipeline. If you would like to
use your own downloading script, you have to use another name of the stage,
except ##[ Download ]##.

The Download stage of ParDim.sh downloads uniques files from the table
and distribute them in right directories, with several options:

  - save files with original names or based on relative name according to
    pattern: relativeName.columnName.extensionOfRelName
    e.g.: relativeName = enc.gz, columnName = ctl => output = enc.ctl.gz
    
    Note: names for relativeName column is not changed
  - combine several files, splited by tabDelimJoin, then
    final name is based on the name of 1st file. If it is relative name,
    then based on 1st name of relative names, if there are several to join.

Since, the Download stage is written for HTCondor environment, it downloads
every file on separate machine simultaneously, which boosts downloading process
in times.

The better description of downloading arguments is provided in Download manual.

