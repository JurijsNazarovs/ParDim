--------------------------------------------------------------------------------
[Introduction]
--------------------------------------------------------------------------------
ParDim - software, which provides the framework (front end) for a simple
procedure of creating pipeline by integrating different stages and paralleling
the execution for a number of analyzed directories, based on HTCondor
environment.

In other words, if you have a script to run on condor for one directory,
which corresponds to a specific data set or experiment, ParDim implements runs
for multiple directories.
Note: more about script is in section [Script designing for ParDim].

ParDim provides tools to manage a workflow of scripts, download and prepare data
for runs (section [Built-in Download stage]), and a great reporting function to
get status of running jobs (section [ParDim installation and execution]).

Ð¡onstruction of a pipeline is completed in an intuitive description of
stages in a text file, with ability to include/exclude a specific stage by
changing a value of just one argument - execute. Such structure allows to
keep all information in one text file and controll the workflow without
deleting chunks of parameters. As an example, pipeline with stage1 -> stage3
looks like:

--------------------
##[ stage1 ]##
execute    true
...
##[ stage2 ]##
execute    false
...
##[ stage3 ]##
execute    true
...
--------------------

ParDim provides all necessary information about a previous stage of pipeline
to a next stage using text files. It controls that results of astage are
transferred in a right directory in an organized way (according to a stage name).

--------------------------------------------------------------------------------
[ParDim installation and execution]
--------------------------------------------------------------------------------
----------------------------------------
*INSTALLATION*
----------------------------------------
1. To install the ParDim go in the directory where you would like to install
   software:
   $ softDir="/path/to/softwareDirectory"
   $ cd "$softDir"
2. Download the current version from the GitHub repository:
   $ git clone git@github.com:JurijsNazarovs/ParDim.git
3. Add following lines in your .bash_profile file (should be in your $HOME path):
   softDir="/path/to/softwareDirectory"
   export PATH="$softDir/ParDim/:$PATH"

Now you have an access to 3 function from any directory:
1. ParDim.sh - ParDim framework
2. MakeArgsFile.sh - creates a template for an argument file
3. MakeReport.sh - provides an information about pipeline running status

----------------------------------------
*EXECUTION*
----------------------------------------
In this subsection we assumne that you did actions described in the subsection
*INSTALLATION* above
--------------------
ParDim.sh
--------------------
ParDim.sh is a bash script and executed as:
ParDim.sh "argsFile" "isSubmit"

1. argsFile - path to a text file with constructed pipeline and relative
   arguments.

   Note: default value is args.list in a root directory of ParDim.sh
2. isSubmit  - boolean variable with values true/false.
   If value is false, then everything is prepared to run the pipeline, but
   does not run, to test the structure to make sure everything is ok.

   Note: default value is true, which means run the whole pipeline.

--------------------
MakeArgsFile.sh
--------------------
MakeArgsFile.sh is a bash script and executed as:
MakeArgsFile.sh "argsFile" "isAppend" "stage1" ... "stageN"

1. argsFile - path to a text file with constructed pipeline and relative
   arguments.

   Note: default value is args.list in a root directory of ParDim.sh
2. isAppend - boolean variable with values true/false.
   true  - append to existing argsFile without creating a head for ParDim.sh
   false - rewrite the whole file
3-. different names of stages in order of required execution

   Note: default value is just ParDim stage, to create necessary arguments
   to run ParDim. More details in section [ParDim Stage].

Note: MakeArgsFile.sh creates a template for every stage, which has to be
filled by user.
Note: for the stage with name Download, ParDim creates different set of
arguments, since it is a built in function.

--------------------
MakeReport.sh
--------------------
MakeReport.sh is a bash script and executed as:
MakeReport.sh "stageName" "jobsDir" "reportDir" "holdReason" "delim"

1. stageName - name of the stage of which to get summary
2. jobsDir - the working directory for the task, specified in ParDim
3. reportDir - directory to create all report files. Default is report
4. holdReason - reason for holding jobs, e.g. "" - all hold jobs, "72 hrs"
5. delim - delimeter to use for output files

The output of MakeReport.sh is 8 files:
1. *.queuedJobs.list - queued jobs
2. *.compJobs.list - completed jobs
3. *.notCompJobs.list - currently not completed jobs
4. *.holJobsReason.list - holding lines given reason $holdReason
5. *.holdJobs.list - jobs on hold given reason $holdReason
6. *.summaryJobs.list - summary info about directories
7. *.notCompDirs.list - path to not completed directories
8. *.compDirs.list - path to completed directories


--------------------------------------------------------------------------------
[Pipeline construction]
--------------------------------------------------------------------------------
To construct a pipeline - combination of stages, the argument file (text file)
is filled with specific syntax for a stage description.

----------------------------------------
*REQUIRED INPUT*
----------------------------------------
There are 3 required arguments to describe a pipeline stage
  1. stageName - provided in a pattern (*S* - any number of spaces):
     *S*##[*S*stageName*S*]##*S*
     
     Note: stageName contains no spaces!
  2. execute - boolean variable with values true/false. Indicates, if a stage
     should be executed or not. If the stage is not executed,
     it is just skipped. So, there is no need to delete it from the argument file.

     Note: if value is not true, task is not executed and warning appears.
  3. script - path to the script, which is executed to construct a DAG.
  
     Note: if not full path is provided, then path is assumed to be relative to a
     directory, where ParDim is executed.
     
----------------------------------------
*ADDITIONAL INPUT*
----------------------------------------
  1. map - argument with 2 values (now): multi/single. Tells what is the type
     of a stage. (more examples later)
  2. args - path to argument file, which is used for a script.
     If the argument is empty, then the current file with description of the
     pipeline is used for a script.
     That means, that in the description of the stage you can provide
     all other necessary information for the script, or create another file
     and provide path to that file here.
     
     Note: if not full path is provided, then path is assumed to be relative to a
     directory, where ParDim is executed.
  3. transFiles - path to all files, splited by comma (and spaces), which are used by
     a script. For example, if you have some libraries to send, you can
     add them here.
     
     Note: if not full path is provided, then the dirname of script is
     considered as a root path of transFiles.

     Note: Files corresponding to arguments "args" and "script" are transferred
     automatically.
  4. relResPath - path for results relative to the part of the pipeline or
     dataPath.
     Possible values are: previous/next stageName and dataPath.
     That is, results for current stage are saved in results directory of a
     stage specified in relResPath.
     Example: There is data in dataPath, and we need to create new additional
     data in same directory. So, that next stage can read original + new data
     from dataPath (or any stages). Then relResPath is dataPath.

     Note: if nothing is provided, then relResPath is resPath/stageName

Note: ParDim.sh provides an error if something is wrong with scripts, args,
transFiles, or relResPath.

----------------------------------------
*EXAMPLE* with all possible arguments
----------------------------------------
##[ stageName ]##  
execute                 true/false
map                     single/multi
script                  /path/to/script
args                    /path/to/argument/file/for/script
transFiles              /path/to/different/files/to/transfer/using/,
relResPath              


--------------------------------------------------------------------------------
[ParDim Stage]
--------------------------------------------------------------------------------
In argsFile, there are 4 available arguments for the ParDim to initialize
the pipeline. Variables have to be specified after the label: ##[ ParDim.sh ]##
   
1. dataPath - path to analyzed data or resulting path for download stage
   (later about that). The path is used to construct a list of analysed
   directories.
2. resPath - path to results of the whole pipeline.
   By default every stage creates its own directory corresponding
   to a stage name. But no default value for resPath.
3. jobsDir - temporary directory for all files generated by ParDim.
   Defaul is to create a temporary unique directory dagTestXXXX .
4. selectJobsListPath - initial list of directories for the analysis.
   Can be empty.

Note: in a majority of cases by providing 2 arguments: resPath and
either dataPath or selectJobsListPath, ParDim works fine and ready to construct
a pipeline. However, there are different possible combinations of arguments,
which are described below.

----------------------------------------
*POSSIBLE COMBINATIONS OF ARGUMENTS*
----------------------------------------
1. Default values - these rules are applied first, so, the values assigned
   here are used in steps 2-4
   - if [jobsDir] is empty, then a directory with unique name dagTmpXXXX
     is created
   - if [resPath] is empty, but [dataPath] is not, then resPath = dirname(dataPath)
   
2. If there is the Download task or [dataPath] is used as [relResPath] for any
   stage, then
   - [dataPath] has to be provided with ability to write
   
3. If first stage is single-mapping, then following argumetns have to be provided:
   - [resPath]
   - [jobsDir]

4. If first stage is multi-mapping, then following argumetns have to be provided:
   - [resPath]
   - [jobsDir]
   - [dataPath] or [selectJobsListPath]


--------------------------------------------------------------------------------
[Script designing for ParDim]
--------------------------------------------------------------------------------
The flow of ParDim is dedicated to 2 steps:
    - construct the DAG file (let call a script to construct a DAG file as
      DagMaker)
    - execute the constructed DAG file
Thus, to design a stage for ParDim, user has to provide a script - DagMaker,
which creates a DAG condor file, and ParDim handles its execution. 

Depending on a mapping value of a stage different arguments are passed to a
DagMaker (but almost the same). Thus, while designing your scripts
(DagMaker for stages), you have to write it in a way to accept these
arguments in the same order. In following subsections details for specific
stages are provided.

Note: DagMaker has to be able to read all necessary argument to  construct a
DAG from the text file. It can be done by using a ParDim function ReadArgs.
More details in the subsection *USEFULL FUNCTIONS*

----------------------------------------
*DIFFERENCE IN ARGUMENTS BETWEEN SINGLE-MAPPING
AND MULTI-MAPPING SCRIPTS*
----------------------------------------
In a muti-mapping script there are two  more arguments compare to a
single-mapping script: resDir and transOut. 

The difference is because ParDim expects that results of a DAG are returned
in a tar file in a specific directory, where later ParDim untar it.

ParDim expects that in single-mapping scripts, the DagMaker developer provides
unique names to tarfiles and directories with results to avoid an overlap of
tar and untar files and directories.

However, since multi-mapping scripts are executed for several directories, to
avoid an overlap ParDim takes care of a directory with results - $resDir,
and a unique name of tar file - $transOut.

----------------------------------------
*SINGLE-MAPPING SCRIPTS*
----------------------------------------
single-mapping scripts are designed to construct a DAG file based on the
specific argument(file), which is most likely has to be pass using args in a
description of a stage.  Usually, it is some sort of prescritps before a
multimapping script.

--------------------
Examples:
--------------------
  1) Download files according to list of links. In this case the args is a
     path to a text file - list of links. The DAG file consists of independent
     jobs, where every job downloads file accordint to a specific link.

     Note: this is a short description of the built-in Download stage of ParDim.
     More details are in a section [Built-in Download stage].
  2) Assume we have a text file with N rows, where N rows contain information
     about M IDs. We would like to analyze every ID separately. In this case the
     args is a path to a text file. The DAG file consists of M independent jobs,
     where every job creates a directory and saves the part of a file
     corresponding to one of M IDs. Thus, creating an input for a multi-mapping
     stage.

--------------------
DagMaker arguments
--------------------
Following arguments are provided for a single-mapping DagMaker script by ParDim:

  1. argsFile - a text file with arguments necessary for a DagMaker.
     This is a file, which you specify as a value in args in a stage description.

     Note: if nothing is specified, the main file with arguments, where the
     structure of a pipeline is described, is used.
  2. dagFile - a name of a DAG file which is submit in a second step. DagMaker
     has to use a value of this variable exactly, without any manipulations.
     For example, to create a file just use: printf "" > "$dagFile".

     Note: ParDim returns the file to a submit server.
  3. jobsDir - a working directory where DagMaker saves all necessary files
     for DAG submission. DagMaker has to use a value of this variable exactly,
     without taking the full path or any other manipulations.

     Note: ParDim returns the directory to a submit server.
  4. resPath - path where results are saved. Below is a suggested chunk of code
     for a DagMaker to save results in the "$resPath":
     --------------------
     conFile="$jobsDir/Name.condor"
     jobId="$uniqJobId" #can be an iterator to count jobs
     
     printf "JOB  $jobId $conFile\n" >> "$dagFile"
     printf "VARS $jobId transOut=\"$jobId.tar.gz\"\n" >> "$dagFile"
     printf "VARS $jobId transMap=\"\$(transOut)=$resPath/\$(transOut)\"\n"\
     >> "$dagFile"
     --------------------
     Note: structure of a condor file "$conFile" and an executed file is
     provided in a subsection *STRUCTURE OF A CONDOR FILE* and
     *STRUCTURE OF AN EXECUTION FILE* respectively.

     Note: value of transOut has to be passed to an execution file and directory
     with resutls has to be tared with a name corresponding to a value of
     transOut.
     
  5. selectJobsListInfo - file with all information about results of a previous
     stage, if a previous stage exists.

     Note: Structure of file is provided in the subsection
     *STRUCTURE OF selectJobsListInfo AND inpDataInfo*
     
----------------------------------------
*MULTI-MAPPING SCRIPTS*
----------------------------------------
Multi-mapping scripts are designed to construct a DAG file based on information
about a specific directory, so that script is executed for every of analyzed
directories independently.

--------------------
Examples:
--------------------
  1) alignment of fastq files for an experiment
  2) peak calling for an experiment
  3) trimming for an experiment

--------------------
DagMaker arguments
--------------------
Following arguments are provided for a multi-mapping DagMaker script by ParDim:

  1. argsFile - a text file with arguments necessary for a DagMaker.
     This is a file, which you specify as a value in args in a stage description.

     Note: if nothing is specified, the main file with arguments, where the
     structure of a pipeline is described, is used.
  2. dagFile - a name of a DAG file which is submit in a second step. DagMaker
     has to use a value of this variable exactly, without any manipulations.
     For example, to create a file just use: printf "" > "$dagFile".

     Note: ParDim returns the file to a submit server.
  3. jobsDir - a working directory where DagMaker saves all necessary files
     for DAG submission. DagMaker has to use a value of this variable exactly,
     without taking the full path or any other manipulations.

     Note: ParDim returns the directory to a submit server.
  4. resPath - path where results are saved. Below is a suggested chunk of code
     for a DagMaker to save results in the "$resPath":
     --------------------
     conFile="$jobsDir/Name.condor"
     jobId="$uniqJobId" #can be an iterator to count jobs
     
     printf "JOB  $jobId $conFile\n" >> "$dagFile"
     printf "VARS $jobId transOut=\"$transOut.jobId.tar.gz\"\n" >> "$dagFile"
     printf "VARS $jobId transMap=\"\$(transOut)=$resPath/\$(transOut)\"\n"\
     >> "$dagFile"
     --------------------
     Note: structure of a condor file "$conFile" and an executed file is
     provided in a subsection *STRUCTURE OF A CONDOR FILE* and
     *STRUCTURE OF AN EXECUTION FILE* respectively.

     Note: value of transOut has to be passed to an execution file and directory
     with resutls has to be tared with a name corresponding to a value of
     transOut.
     
     Note: in contrast to a single-mapping script the transOut variable depends on
     a $transOut variable, which is described under number 7 in the current list.
     
  5. inpDataInfo - file with all information about results of a previous
     stage for the same directory (specific analyzed directory and not for
     all of them as in single script).

     Note: Structure of file is provided in the subsection
     *STRUCTURE OF selectJobsListInfo AND inpDataInfo*.

  Following arguments are provided to a DagMaker just to pass them further in an
  execution file.
  6. resDir - directory to save results, equals to a name of analysed directory.
  DagMaker has to pass a value of this variable exactly, without taking the full
  path or any other manipulations and should be used same way in an execution
  file.
  7. transOut - an unique name of a tarfile to tar $resDir in an execution file.

----------------------------------------
*STRUCTURE OF A CONDOR FILE*
----------------------------------------
In both cases of maping scripts we specify transOut and transMap as variables
for a specific job. So, "$conFile" description is:

...
transfer_output = $(transOut)
transfer_output_remaps = "$(transMap)"
arguments = "'arg1', 'arg2', '\$(transOut)', ...,
             'resDir - in case of multi-mapping script'"
...

----------------------------------------
*STRUCTURE OF AN EXECUTION FILE*
----------------------------------------
In an execution file results have to be tared with a name corresponding
to a value of transOut variable. That is, in an execution file you have to have
a variable like outTar=$3, where 3 - corresponds to a position of transOut in 
arguments in condor file (see above). Then do:

tar -czf "$outTar" "$dirWithAllresults ($resDir in case of multi-mapping script)"

----------------------------------------
*STRUCTURE OF a selectJobsListInfo
AND an inpDataInfo*
----------------------------------------
The difference between a selectJobsListInfo and an InpDataInfo is that first file
is a combination of a second file but for all directories in a resulting
directory of a previous stage.

While the InpDataInfo contains information from a previous stage just about
single analysed directory. The InpDataInfo structure is:

/path/to/directory/or/subdirectory:
fileName1        size in bytes
....
fileNameN        size in bytes

Note: the space above is '\t' - tabular.

----------------------------------------
*USEFULL FUNCTIONS*
----------------------------------------
--------------------
ReadArgs
--------------------
Location: ParDim/scripts/funcListParDim.sh
ReadArgs - reads arguments from a file in a section according to a label
##[ scrLab ]## and substitute values in a code.

For example, in an argument file we have a line: foo     23
Then in a code after function ReadArgs is called, "echo $foo" returns 23.

Input:
 1. argsFile - file with arguments
 2. scrLabNum - number of script labels
 3. scrLabList - vector of names of labels (scrLab) to search for arguments
    according to the patern: ##[    scrLab  ]## - Case sensetive. Might be
    spaces before, after and inside, but cannot split scrLab, ##[, and ]##.
    If scrLab = "", the whole file is searched for arguments, and the last
    entry is selected.
 4. posArgNum - number of arguments to read
 5. posArgList - possible arguments to search for
 6. reservArg  - reserved argument which can't be duplicated
 7. isSkipLabErr - binary variable true/false. If true, then no error appeared
    for missed labels, if other labels exist. No arguments are read.

Possible behavior:
- If a variable is defined before reading file, and in a file it is empty,
  then original value remains.
- If an argument repeats several times, then warning appears and last value is
  assigned.
- If an argument is empty after reading a file, a warning appears.
- If no labels are provided, the whole file is read.
- If label appears several times, then error.
- If label cannot be found while other labels exist, then error (or not if
  isSkipLabErr = true)
- If a file has no sections (scrLab), then the whole file is read
    
How to write an argsFile:
argumentName(no spaces) argumentValue(spaces, tabs, any symbols)
That is, after first column space has to be provided!

--------------------
PrintArgs
--------------------
Location: ParDim/scripts/funcListParDim.sh
PrintArgs - print arguments for a specific script in a beautifull way.

Input:
 1. scrName - name of a script to print arguments for. Can be an empty. It just
    influences a header message.
 2-. posArgs - vector of arguments to print values of.

Usage: PrintArgs "$scriptName" "${posArgs[@]}"

--------------------
makeCon.sh
--------------------
Location: ParDim/scripts/makeCon.sh
makeCon.sh - creates a condor submit file, depending on parameters.

Check input in the file in section "Input".


--------------------------------------------------------------------------------
[Built-in Download stage]
--------------------------------------------------------------------------------
To download files (to fill a dataPath) ParDim provides a stage Download.

The Download stage of the ParDim downloads uniques files from a table and
distribute them in right directories. The Download stage is written for HTCondor
environment, it downloads every file on separate machine simultaneously, which
boosts downloading process in times.

The Download stage provides several usefull options: 
  - save files with original names or based on relative name according to the
    pattern: "relativeName.columnName.extensionOfRelativeName"
    e.g.: relativeName = enc.gz, columnName = ctl => output = enc.ctl.gz
    
    Note: names for relativeName column is not changed
  - combine several files in one output

Note: the name of the stage is reserved for a built-in ParDim.sh script
boostDownload.sh. If you would like to use your own downloading script, you
have to use another name of the stage, except ##[ Download ]##.

----------------------------------------
The structure of an input table:
----------------------------------------
One of columns contains basename of directories, where files are supposed to be
saved. The number of column is set in an argument tabDirCol.

Other columns contain URLs to download files. One column corresponds to a
specific type of downloading files. For example, column of control files.

To combine files in one output, links in a column have to be split by a value
in an argument tabDelimJoin.

Since Download stage is developed for HTCondor environment, it is necessary to
detect size for downloading files. The Download stage does it automatically,
but if this information is provided in a table, then argument tabIsSize should
be set as true.

----------------------------------------
Input in a file with arguments:
----------------------------------------
 1. tabPath - path to an input table with links to files.
 2. tabDelim - delimeter used in a table.
               Default value is ','.		
 3. tabDelimJoin - delimeter to use in a table to join files.
                   Default value is ';'.
 4. tabDirCol - index of column with directory.
                Default value is 1.
 5. tabIsOrigName - true/false, Indicates if use original names or not.
                    Default value is false.
 6. tabRelNameCol - column to use as a base for names if tabOrigName=false.
                    Default value is 2.
 7. tabIsSize - true/false. Indicates if table has size of files or not.
                Default value is false.
 8. nDotsExt - number of dots before extension of download files starts.
               Default value is 1.
--------------------------------------------------------------------------------
